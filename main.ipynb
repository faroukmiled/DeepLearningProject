{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "c097a618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" text = load_text()[:100]\\nvocab, char2idx, idx2char = create_vocab(text)\\nint_text = text_to_int(text, char2idx)\\ntext_int = int_to_text(int_text, idx2char)\\ndataLoad = create_dataset(int_text, 20, 50)\\nfor batch in dataLoad:\\n    inputs, targets = batch\\n    for inp, trg in zip(inputs,targets):\\n        print('--------------------\\n')\\n        print(f'The input: {int_to_text(inp, idx2char)}, corresponds to the output {int_to_text(trg, idx2char)}\\n')\\n        print('--------------------\\n')  \""
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_text(file_path = 'shakespeare.txt'):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def create_vocab(text):\n",
    "    vocab = sorted(set(text))\n",
    "    char2idx = {char_i:i for i, char_i in enumerate(vocab)}\n",
    "    idx2char = np.array(vocab)\n",
    "    return vocab, char2idx, idx2char\n",
    "\n",
    "def text_to_int(text, char2idx):\n",
    "    return np.array([char2idx[i] for i in text])\n",
    "\n",
    "def int_to_text(index, idx2char):\n",
    "    return ''.join(idx2char[index])\n",
    "\n",
    "def create_dataset(text_as_int, seq_length, batch_size):\n",
    "    total_num_seq = len(text_as_int) - seq_length\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    for i in range(0,total_num_seq):\n",
    "        inputs.append(text_as_int[i:i+seq_length])\n",
    "        targets.append(text_as_int[i+1:i+seq_length+1])\n",
    "    \n",
    "    inputs = torch.tensor(np.array(inputs))\n",
    "    targets = torch.tensor(np.array(targets))\n",
    "    \n",
    "    dataset = torch.utils.data.TensorDataset(inputs, targets)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "# Testing code\n",
    "\"\"\" text = load_text()[:100]\n",
    "vocab, char2idx, idx2char = create_vocab(text)\n",
    "int_text = text_to_int(text, char2idx)\n",
    "text_int = int_to_text(int_text, idx2char)\n",
    "dataLoad = create_dataset(int_text, 20, 50)\n",
    "for batch in dataLoad:\n",
    "    inputs, targets = batch\n",
    "    for inp, trg in zip(inputs,targets):\n",
    "        print('--------------------\\n')\n",
    "        print(f'The input: {int_to_text(inp, idx2char)}, corresponds to the output {int_to_text(trg, idx2char)}\\n')\n",
    "        print('--------------------\\n')  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "c96eb684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create all the basis for the RNN architecture implemented with PyTorch\n",
    "# The size of the feature in nn.RNN(hidden_size, feature) was selected as 'hidden size'\n",
    "#       for simplicity\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, seq_length):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_legth = seq_length\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "0ed55c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(seq_length, batch_size, path_file = 'shakespeare.txt', amount_chars = None):\n",
    "    if amount_chars:\n",
    "        text = load_text(path_file)[:amount_chars]\n",
    "    else:\n",
    "        text = load_text(path_file)\n",
    "    print(f'Text of len {len(text)} is being processed.\\n')\n",
    "    vocab, char2idx, idx2char = create_vocab(text)\n",
    "    text_as_int = text_to_int(text, char2idx)\n",
    "    dataloader = create_dataset(text_as_int, seq_length, batch_size)\n",
    "\n",
    "    return dataloader, vocab, char2idx, idx2char, text_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "b36ab6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how this architecture works\n",
    "\n",
    "'''\n",
    "seq_length = 100\n",
    "batch_size = 64\n",
    "hidden_size = 128\n",
    "epochs = 5\n",
    "learning_rate = 0.003\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "'''\n",
    "\n",
    "def train_RNN(seq_length, batch_size, hidden_size, epochs, learning_rate, device, amount_chars = None):\n",
    "    \n",
    "    # Get data\n",
    "    dataloader, vocab, char2idx, idx2char, text_as_int = get_dataloader(seq_length, batch_size, amount_chars= amount_chars)\n",
    "\n",
    "    # Model\n",
    "    model = CharRNN(len(vocab), hidden_size, seq_length).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f'Training the RNN vanilla network.')\n",
    "\n",
    "    initial_run_time = time.time()\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            hidden = model.init_hidden(batch_size).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, hidden = model(x_batch, hidden)\n",
    "            loss = criterion(output.view(-1, len(vocab)), y_batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, batch duration {time.time() - start_time:.2f} seconds.\\n')\n",
    "    \n",
    "    print(f'Total training time {time.time()-initial_run_time:.2f}.\\n')\n",
    "\n",
    "\n",
    "    return model, char2idx, idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "afd6883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to evaluate the model\n",
    "\n",
    "def generat_text(model, start_string, char2idx, idx2char, length = 200, device = 'cpu', is_lstm = False):\n",
    "\n",
    "    model.eval()\n",
    "    input_eval = torch.tensor([char2idx[i] for i in start_string]).unsqueeze(0).to(device)\n",
    "\n",
    "    if not is_lstm:\n",
    "        hidden = model.init_hidden(1).to(device)\n",
    "    elif is_lstm:\n",
    "        hidden = model.init_hidden(1, device)\n",
    "\n",
    "    generated = list(start_string)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(length):\n",
    "\n",
    "            output, hidden = model(input_eval, hidden)\n",
    "            logits = output[:,-1, :] # In this line we can add temperature\n",
    "            probs = torch.softmax(logits, dim = 1).squeeze()\n",
    "\n",
    "            next_idx = torch.multinomial(probs,1).item()\n",
    "            next_char = idx2char[next_idx]\n",
    "\n",
    "            generated.append(next_char)\n",
    "\n",
    "            input_eval = torch.tensor([[next_idx]]).to(device)\n",
    "    \n",
    "    return ''.join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "31e8d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 10\n",
    "batch_size = 20\n",
    "hidden_size = 128\n",
    "epochs = 5\n",
    "learning_rate = 0.003\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "15058013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text of len 10000 is being processed.\n",
      "\n",
      "Training the RNN vanilla network.\n",
      "Epoch 1/5, Loss: 1.7044, batch duration 0.66 seconds.\n",
      "\n",
      "Epoch 2/5, Loss: 1.5352, batch duration 0.63 seconds.\n",
      "\n",
      "Epoch 3/5, Loss: 1.5348, batch duration 0.61 seconds.\n",
      "\n",
      "Epoch 4/5, Loss: 1.4427, batch duration 0.62 seconds.\n",
      "\n",
      "Epoch 5/5, Loss: 1.4925, batch duration 0.59 seconds.\n",
      "\n",
      "Total training time 3.12.\n",
      "\n",
      "\n",
      "Generated text:\n",
      "\n",
      "ROMEO: you do plespor'd shop,\n",
      "Your moremay am! you musts and that down cit toe.\n",
      "Hang aboud dearter But even up and pllobble coof me who cive\n",
      "But is nos know appleat I dourselves. What he is a kind guly: they\n"
     ]
    }
   ],
   "source": [
    "# Train the vanilla RNN model\n",
    "model_rnn, char2idx, idx2char = train_RNN(seq_length, batch_size, hidden_size, epochs, learning_rate, device, amount_chars=10000)\n",
    "text = generat_text(model_rnn, start_string='ROMEO: ', char2idx= char2idx, idx2char=idx2char, device=device)\n",
    "print(\"\\nGenerated text:\\n\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "d6031809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create all the basis for the RNN architecture implemented with PyTorch\n",
    "# The size of the feature in nn.RNN(hidden_size, feature) was selected as 'hidden size'\n",
    "#       for simplicity\n",
    "\n",
    "class CharRNN_with_Temperature(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, seq_length,T):\n",
    "        super(CharRNN_with_Temperature, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_legth = seq_length\n",
    "        self.T = T\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)/self.T\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how this architecture works\n",
    "\n",
    "'''\n",
    "seq_length = 100\n",
    "batch_size = 64\n",
    "hidden_size = 128\n",
    "epochs = 5\n",
    "learning_rate = 0.003\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "'''\n",
    "\n",
    "def train_RNN_with_Temperature_scaling(seq_length, batch_size, hidden_size,T, epochs, learning_rate, device, amount_chars = None):\n",
    "    \n",
    "    # Get data\n",
    "    dataloader, vocab, char2idx, idx2char, text_as_int = get_dataloader(seq_length, batch_size, amount_chars= amount_chars)\n",
    "\n",
    "    # Model\n",
    "    model = CharRNN_with_Temperature(len(vocab), hidden_size, seq_length,T).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f'Training the RNN vanilla network with temperatue scaling .')\n",
    "\n",
    "    initial_run_time = time.time()\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            hidden = model.init_hidden(batch_size).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, hidden = model(x_batch, hidden)\n",
    "            loss = criterion(output.view(-1, len(vocab)), y_batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, batch duration {time.time() - start_time:.2f} seconds.\\n')\n",
    "    \n",
    "    print(f'Total training time {time.time()-initial_run_time:.2f}.\\n')\n",
    "\n",
    "\n",
    "    return model, char2idx, idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text of len 10000 is being processed.\n",
      "\n",
      "Training the RNN vanilla network with temperatue scaling .\n",
      "Epoch 1/5, Loss: 2.0851, batch duration 0.67 seconds.\n",
      "\n",
      "Epoch 2/5, Loss: 1.8266, batch duration 0.62 seconds.\n",
      "\n",
      "Epoch 3/5, Loss: 1.5535, batch duration 0.62 seconds.\n",
      "\n",
      "Epoch 4/5, Loss: 1.6689, batch duration 0.60 seconds.\n",
      "\n",
      "Epoch 5/5, Loss: 1.5753, batch duration 0.59 seconds.\n",
      "\n",
      "Total training time 3.10.\n",
      "\n",
      "\n",
      "Generated text:\n",
      "\n",
      "ROMEO: sir; wherained; hee will olly.\n",
      "\n",
      "MENENIUS:\n",
      "The stinous, killy. was evil.\n",
      "\n",
      "MENENIUS:\n",
      "Their rus, evil. He the ot, the belly's,\n",
      "What are seeesond felay. But, I am thirflubs,\n",
      "That wark mished agaings a thu\n"
     ]
    }
   ],
   "source": [
    "# Train the vanilla RNN model\n",
    "model_rnn, char2idx, idx2char = train_RNN_with_Temperature_scaling(seq_length, batch_size, hidden_size, 0.1, epochs, learning_rate, device, amount_chars=10000)\n",
    "text = generat_text(model_rnn, start_string='ROMEO: ', char2idx= char2idx, idx2char=idx2char, device=device)\n",
    "print(\"\\nGenerated text:\\n\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text of len 10000 is being processed.\n",
      "\n",
      "Training the RNN vanilla network with temperatue scaling .\n",
      "Epoch 1/5, Loss: 1.7806, batch duration 0.61 seconds.\n",
      "\n",
      "Epoch 2/5, Loss: 1.4406, batch duration 0.60 seconds.\n",
      "\n",
      "Epoch 3/5, Loss: 1.3916, batch duration 0.59 seconds.\n",
      "\n",
      "Epoch 4/5, Loss: 1.2649, batch duration 0.59 seconds.\n",
      "\n",
      "Epoch 5/5, Loss: 1.3443, batch duration 0.60 seconds.\n",
      "\n",
      "Total training time 2.99.\n",
      "\n",
      "\n",
      "Generated text:\n",
      "\n",
      "ROMEO: the store-hould hele down are use the pion one to eac, you haths. He receive him first Citizen:\n",
      "Well:\n",
      "Speak thrighs granted ther?\n",
      "\n",
      "Second Citizen:\n",
      "Conjrates, my gide him,\n",
      "Sexate, you munterflicts here\n"
     ]
    }
   ],
   "source": [
    "# Train the vanilla RNN model\n",
    "model_rnn, char2idx, idx2char = train_RNN_with_Temperature_scaling(seq_length, batch_size, hidden_size, 0.95, epochs, learning_rate, device, amount_chars=10000)\n",
    "text = generat_text(model_rnn, start_string='ROMEO: ', char2idx= char2idx, idx2char=idx2char, device=device)\n",
    "print(\"\\nGenerated text:\\n\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to evaluate the model\n",
    "\n",
    "def generat_text_nucleus_sampling(model, start_string, char2idx, idx2char, length = 200, device = 'cpu', is_lstm = False):\n",
    "\n",
    "    model.eval()\n",
    "    p = 0.95\n",
    "    input_eval = torch.tensor([char2idx[i] for i in start_string]).unsqueeze(0).to(device)\n",
    "\n",
    "    if not is_lstm:\n",
    "        hidden = model.init_hidden(1).to(device)\n",
    "    elif is_lstm:\n",
    "        hidden = model.init_hidden(1, device)\n",
    "\n",
    "    generated = list(start_string)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(length):\n",
    "\n",
    "            output, hidden = model(input_eval, hidden)\n",
    "            logits = output[:,-1, :] # In this line we can add temperature\n",
    "            probs = torch.softmax(logits, dim = 1).squeeze()\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            sorted_probs = F.softmax(sorted_logits,dim=-1)\n",
    "            new_probs = torch.zeros_like(probs)\n",
    "            cum = 0\n",
    "            index = 0\n",
    "            for v in range(sorted_logits.size(1)):\n",
    "                cum+=sorted_probs[0,v:v+1]\n",
    "                index=v\n",
    "                if (cum>=p):\n",
    "                    break\n",
    "            new_probs[sorted_indices[0,:index+1]]=probs[sorted_indices[0,:index+1]]/cum\n",
    "            #print(new_probs)\n",
    "\n",
    "            next_idx = torch.multinomial(new_probs,1).item()\n",
    "            next_char = idx2char[next_idx]\n",
    "\n",
    "            generated.append(next_char)\n",
    "\n",
    "            input_eval = torch.tensor([[next_idx]]).to(device)\n",
    "    \n",
    "    return ''.join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create all the basis for the RNN architecture implemented with PyTorch\n",
    "# The size of the feature in nn.RNN(hidden_size, feature) was selected as 'hidden size'\n",
    "#       for simplicity\n",
    "\n",
    "class CharRNN_with_Nucleus_Sampling(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, seq_length,p):\n",
    "        super(CharRNN_with_Nucleus_Sampling, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_legth = seq_length\n",
    "        self.p = p\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        logits = self.fc(out)\n",
    "\n",
    "\n",
    "        return logits, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_RNN_with_nucleus_sampling(seq_length, batch_size, hidden_size,p, epochs, learning_rate, device, amount_chars = None):\n",
    "    \n",
    "    # Get data\n",
    "    dataloader, vocab, char2idx, idx2char, text_as_int = get_dataloader(seq_length, batch_size, amount_chars= amount_chars)\n",
    "\n",
    "    # Model\n",
    "    model = CharRNN_with_Nucleus_Sampling(len(vocab), hidden_size, seq_length,p).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f'Training the RNN vanilla network with temperatue scaling .')\n",
    "\n",
    "    initial_run_time = time.time()\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            hidden = model.init_hidden(batch_size).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, hidden = model(x_batch, hidden)\n",
    "            loss = criterion(output.view(-1, len(vocab)), y_batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, batch duration {time.time() - start_time:.2f} seconds.\\n')\n",
    "    \n",
    "    print(f'Total training time {time.time()-initial_run_time:.2f}.\\n')\n",
    "\n",
    "\n",
    "    return model, char2idx, idx2char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text of len 100000 is being processed.\n",
      "\n",
      "Training the RNN vanilla network with temperatue scaling .\n",
      "Epoch 1/5, Loss: 1.6557, batch duration 6.04 seconds.\n",
      "\n",
      "Epoch 2/5, Loss: 1.7329, batch duration 6.03 seconds.\n",
      "\n",
      "Epoch 3/5, Loss: 1.6524, batch duration 6.03 seconds.\n",
      "\n",
      "Epoch 4/5, Loss: 1.6755, batch duration 5.74 seconds.\n",
      "\n",
      "Epoch 5/5, Loss: 1.6929, batch duration 5.96 seconds.\n",
      "\n",
      "Total training time 29.79.\n",
      "\n",
      "\n",
      "Generated text:\n",
      "\n",
      "ROMEO: be masted, I carry to armse senate well, no, hear are do that cannot wing.\n",
      "\n",
      "Second Senator:\n",
      "Conselfery make shall wards good ray!\n",
      "\n",
      "CORIOLANUS:\n",
      "We conole rece?\n",
      "\n",
      "CORIOLANUS:\n",
      "O gare, and catccoveral, sir\n"
     ]
    }
   ],
   "source": [
    "# Train the vanilla RNN model\n",
    "model_rnn, char2idx, idx2char = train_RNN_with_nucleus_sampling(seq_length, batch_size, hidden_size, 0.8, 5, learning_rate, device, amount_chars=100000)\n",
    "text = generat_text_nucleus_sampling(model_rnn, start_string='ROMEO: ', char2idx= char2idx, idx2char=idx2char, device=device)\n",
    "print(\"\\nGenerated text:\\n\")\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
